---
description: Optimized GPT-2 with Reinforcement Learning from Human Feedback
title: RLHF-GPT2
toc: true
badges: true
comments: true
author: Sushant P.
categories: [rlhf, nlp, gpt2]
image: images/posts/instructgpt.png
cover: images/covers/instructgpt.png
layout: notebook
permalink: /blog/:year:month:day/rlhfgpt2
sticky_rank: 1
---
<p>This post was originally published on the <a href="https://medium.com/@sushant.pargaonkar97/accelerating-bert-inference-with-lora-and-tensorrt-edit-d6beb2add24d">Medium ML Blog</a></p>

<h1 id="1.-Introduction">1. Introduction<a class="anchor-link" href="#1.-Introduction"> </a>
<p>ChatGPT is one the robust chatbot that is avaible in present stage and it is changing our work and lifestyle. Based on which there are many RAG based chatbots developed on top of LLM api. The main challenge in these systems is how can you extract the knowledge present in LLM for your use case. The part of these LLMs perform so well to the human understanding is because of human feedback in finetuing stage called RLHF. RLHF allgins LLM output to be coherent with the user query which makes the model very powerful However, the question we ask here is Can we align smaller models to also behavior reasonably in a dialogue similar with large language model such as GPT-3 175B?</p>

<p>We implemented GPT architecture with casual language model training by loosely follow Andrej's implementation of nanoGPT. The initial model weights of GPT2 are loaded from Huggingface. We also added Low-rank Approximation to the linear layers inside the multi-head attention and to the projection layer.</p>

<p>For SFT, we did autoregressive style pre-training with cross entropy loss on next tokens using  half of Anthropic HH-RLHF dataset as train set. Reward Model is done follwiing th InstructGPT and other Half of Anthropic HH-RLHF dataset is used as train and test set. The reward model has a transformer decoder layers as backbone, and used a logistic output to replace the the N-way language model head. Based on InstructGPT PPO is implemented with Actor-Critic.</p>

<p>We achive 4x speed up over BERT base model with minimal(approx 0.4%) loss in accuracy. The code is documented and published on <a href="https://github.com/sushant-97/Optimizing-GPT-2-with-Reinforcement-Learning-from-Human-Feedback">GitHub</p>


<h1 id="2.-Full Post">2. Full Post<a class="anchor-link" href="#2.-Full Post"> </a>
<p>... <a href="https://github.com/sushant-97/Optimizing-GPT-2-with-Reinforcement-Learning-from-Human-Feedback"> (continue reading here)</a></p>